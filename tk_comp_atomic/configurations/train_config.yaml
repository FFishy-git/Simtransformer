batch_size: 64
lr_scheduler: cosine
cosine_scheduler_config:
  lr_decay_steps: 100000000.0
  min_lr: 1.0e-05
  warmup_steps: 1000
StepLR_scheduler_config:
  gamma: 0.1
  step_size: 1000
learning_rate: 0.001
max_epochs: 500
optimizer: AdamW
wandb_config:
  wandb_entity: scgpt-siyu
  wandb_project: composition_v2
weight_decay: 0.008
loss_type: cross_entropy
seed: 1
